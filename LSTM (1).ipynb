{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OXsWrj8EFzA"
      },
      "outputs": [],
      "source": [
        "#importing the necessary libraries and dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns;\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import time\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('val%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('val%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('val%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sa9bpRWEsgR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHhq-x9wEFzD"
      },
      "outputs": [],
      "source": [
        "# loading the data into the dataframe\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/AQI/Data.csv', header=0, index_col=0)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgbTm5qSeAnU"
      },
      "outputs": [],
      "source": [
        "# dataset.rename(columns = {'Unnamed: 0':'ID'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En2uQUkREFzI"
      },
      "outputs": [],
      "source": [
        "# #number of rows and columns in the dataset\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJsQJXj7EFzH"
      },
      "outputs": [],
      "source": [
        "dataset=dataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsY0soyiEFzJ"
      },
      "outputs": [],
      "source": [
        "# #statistical information about columns\n",
        "print(dataset.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsOa8rRAXaCa"
      },
      "outputs": [],
      "source": [
        "# df=dataset.describe()\n",
        "# df.to_excel('/content/drive/MyDrive/AQI/Description.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LNpOVTpJi-2"
      },
      "outputs": [],
      "source": [
        "dataset.drop({'wd',}, axis = 1, inplace = True)\n",
        "dataset.drop({'station',}, axis = 1, inplace = True)\n",
        "dataset.drop({'date',}, axis = 1, inplace = True)\n",
        "dataset.drop({'AQI',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'dateInt',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'AQI_numeric',}, axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOKox6t2emqr"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tqj56GYflRg"
      },
      "outputs": [],
      "source": [
        "# dataset = pd.DataFrame(dataset, columns=[\"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\", \"O3\", \"PRES\", \"DEWP\", \"RAIN\", \"WSPM\",])\n",
        "# # dataset.plot.box()\n",
        "# dataset.plot( kind=\"area\", figsize=(15, 12))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLmtSQeQEFzL"
      },
      "outputs": [],
      "source": [
        "# #checking how many null values are in each column\n",
        "# dataset.drop({'wd',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'station',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'date',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'AQI',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'TEMP',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'PRES',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'DEWP',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'RAIN',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'WSPM',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'dateInt',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'AQI_numeric',}, axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udjT5ULcZYUB"
      },
      "outputs": [],
      "source": [
        "# #number of rows and columns in the dataset\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUsf_dBHEFzM"
      },
      "source": [
        "Just doing `df.dropna()` drops all the NaN values only for the current execution of the cell. If you do the above `df.isnull().sum()` now, you can see that null values still persists. You can solve this by assigning the obtained output of\n",
        "`df.dopna()` to the variable `df` which stores our data (dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHQFwd23EFzN"
      },
      "outputs": [],
      "source": [
        "# correlations = dataset.corr()\n",
        "# fig, ax = plt.subplots(figsize=(15,15))\n",
        "# sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
        "# plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xACBpw54FQNW"
      },
      "outputs": [],
      "source": [
        "# g = sns.pairplot(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLe_4GaPqbXZ"
      },
      "outputs": [],
      "source": [
        "# dataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False, figsize=(30,20))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBTKpDgf2MLa"
      },
      "outputs": [],
      "source": [
        "# values = dataset.values\n",
        "# # integer encode direction\n",
        "# encoder = LabelEncoder()\n",
        "# values[:,4] = encoder.fit_transform(values[:,4])\n",
        "# print(values)\n",
        "# pd.DataFrame(values).to_excel('/content/drive/MyDrive/AQI/IntergerEncoding.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UinK_Nrr0L5g"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # ensure all data is float\n",
        "# values = values.astype('float32')\n",
        "# np.savetxt('/content/drive/MyDrive/AQI/my_data.csv', values, delimiter=',')\n",
        "# # values.to_csv(r'/content/drive/MyDrive/AQI/my_data.csv', index=False)\n",
        "# print(values)\n",
        "\n",
        "# pd.DataFrame(values).to_excel('/content/drive/MyDrive/AQI/FloatConversion.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x70GNxZ71H7w"
      },
      "outputs": [],
      "source": [
        "# reframed = series_to_supervised(values, 1, 1)\n",
        "# # split into train and test sets\n",
        "# values = reframed.values\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scaled_features = scaler.fit_transform(values[:,:-1])\n",
        "# scaled_label = scaler.fit_transform(values[:,-1].reshape(-1,1))\n",
        "# values = np.column_stack((scaled_features, scaled_label))\n",
        "# print(values)\n",
        "# pd.DataFrame(values).to_excel('/content/drive/MyDrive/AQI/MinMaxScaling.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQdyySccqfsH"
      },
      "outputs": [],
      "source": [
        "values = dataset.values\n",
        "# integer encode direction\n",
        "encoder = LabelEncoder()\n",
        "values[:,4] = encoder.fit_transform(values[:,4])\n",
        "# ensure all data is float\n",
        "values = values.astype('float32')\n",
        "reframed = series_to_supervised(values, 1, 1)\n",
        "reframed.drop(reframed.columns[[-7,-6,-5,-4,-3,-2,-1]], axis=1, inplace=True)\n",
        "print(reframed.head())\n",
        "\n",
        "# split into train and test sets\n",
        "values = reframed.values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_features = scaler.fit_transform(values[:,:-1])\n",
        "scaled_label = scaler.fit_transform(values[:,-1].reshape(-1,1))\n",
        "values = np.column_stack((scaled_features, scaled_label))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJJJvhrNqjYp"
      },
      "outputs": [],
      "source": [
        "n_train_hours = 365 * 24 * 40\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]\n",
        "# split into input and outputs\n",
        "# features take all values except the var1\n",
        "train_X, train_y = train[:, :-1], train[:, -1]\n",
        "test_X, test_y = test[:, :-1], test[:, -1]\n",
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OkXP3nyJOmf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnbEc08mBBrc"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(17,8))\n",
        "plt.plot(train_y, label='train_y', color='#0096FF',)\n",
        "plt.plot(test_y, label='test_y',color='#1F51FF',)\n",
        "\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "plt.savefig(\"traintest.pdf\")\n",
        "files.download(\"traintest.pdf\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7G0rxqs5niT"
      },
      "outputs": [],
      "source": [
        "# design network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=(1, 18)),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "#keras.optimizers.SGD(lr=0.001, decay=1e-5, momentum=1.0, nesterov=False)\n",
        "#keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True) #good\n",
        "\n",
        "#keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9)\n",
        "\n",
        "keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, amsgrad=False)\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['accuracy'] )\n",
        "tf.keras.utils.plot_model(model=model, show_shapes=True)\n",
        "tf.keras.utils.plot_model(model=model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2A5ppdW8F5w"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNe81oRKqnLQ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "# Fit the network\n",
        "history = model.fit(train_X, train_y, epochs=10, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n",
        "\n",
        "# Plot history\n",
        "plt.figure(figsize=(17, 8))\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions\n",
        "yhat = model.predict(test_X)\n",
        "\n",
        "# Reshape test_X if needed\n",
        "if len(test_X.shape) == 3:\n",
        "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "\n",
        "# Invert scaling for forecast\n",
        "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat = inv_yhat[:, 0]\n",
        "\n",
        "# Invert scaling for actual\n",
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y = inv_y[:, 0]\n",
        "\n",
        "end = time.time()\n",
        "print('This took {} seconds.'.format(end - start))\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "mae = tf.keras.metrics.mean_absolute_error(inv_y, inv_yhat)\n",
        "inv_y = scaler.inverse_transform(inv_y.reshape(-1, 1))\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8lnrwyAvyXp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "# calculate matrics\n",
        "mse = mean_squared_error(test_y, yhat)\n",
        "mae = mean_absolute_error(test_y, yhat)\n",
        "mape = mean_absolute_percentage_error(test_y, yhat)\n",
        "\n",
        "\n",
        "print('MSE: %.5f' % mse)\n",
        "print('MAE : %.5f' % mae)\n",
        "print('MAPE : %.5f' % mape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJhr5-PuFZgL"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(test_y, yhat)\n",
        "print('MSE: %.5f' % mse)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "D4T47wa10c0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G12nNg2Tq6tL"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"LSTMPre.pdf\")\n",
        "    files.download(\"LSTMPre.pdf\")\n",
        "    plt.show()\n",
        "plot_predicted(inv_yhat[:1000,], inv_y[:1000,])\n",
        "print('Root Mean Squared Error: {:.4f}'.format(rmse))\n",
        "print(\"R2 score : %.2f\" % r2_score(inv_y,inv_yhat))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baIkgOjaasMH"
      },
      "outputs": [],
      "source": [
        "# summarize history for loss\n",
        "pyplot.figure(figsize=(17,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.savefig(\"loss.pdf\")\n",
        "files.download(\"loss.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuwZ9tXRtUnZ"
      },
      "outputs": [],
      "source": [
        "t=history.history['val_loss']\n",
        "np.average(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWsX8Oo-bPNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(\"Accuracy.pdf\")\n",
        "files.download(\"Accuracy.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sAVyhCAWs0cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSKfB21Ys0RD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}