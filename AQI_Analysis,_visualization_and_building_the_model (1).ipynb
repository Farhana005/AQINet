{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OXsWrj8EFzA"
      },
      "outputs": [],
      "source": [
        "#importing the necessary libraries and dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns;\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import time\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('val%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('val%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('val%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sa9bpRWEsgR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHhq-x9wEFzD"
      },
      "outputs": [],
      "source": [
        "# loading the data into the dataframe\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/AQI/Data.csv', header=0, index_col=0)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgbTm5qSeAnU"
      },
      "outputs": [],
      "source": [
        "# dataset.rename(columns = {'Unnamed: 0':'ID'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En2uQUkREFzI"
      },
      "outputs": [],
      "source": [
        "# #number of rows and columns in the dataset\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJsQJXj7EFzH"
      },
      "outputs": [],
      "source": [
        "dataset=dataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsY0soyiEFzJ"
      },
      "outputs": [],
      "source": [
        "# #statistical information about columns\n",
        "print(dataset.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsOa8rRAXaCa"
      },
      "outputs": [],
      "source": [
        "# df=dataset.describe()\n",
        "# df.to_excel('/content/drive/MyDrive/AQI/Description.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LNpOVTpJi-2"
      },
      "outputs": [],
      "source": [
        "dataset.drop({'wd',}, axis = 1, inplace = True)\n",
        "dataset.drop({'station',}, axis = 1, inplace = True)\n",
        "dataset.drop({'date',}, axis = 1, inplace = True)\n",
        "dataset.drop({'AQI',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'dateInt',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'AQI_numeric',}, axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOKox6t2emqr"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tqj56GYflRg"
      },
      "outputs": [],
      "source": [
        "# dataset = pd.DataFrame(dataset, columns=[\"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\", \"O3\", \"PRES\", \"DEWP\", \"RAIN\", \"WSPM\",])\n",
        "# # dataset.plot.box()\n",
        "# dataset.plot( kind=\"area\", figsize=(15, 12))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLmtSQeQEFzL"
      },
      "outputs": [],
      "source": [
        "# #checking how many null values are in each column\n",
        "# dataset.drop({'wd',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'station',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'date',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'AQI',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'TEMP',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'PRES',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'DEWP',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'RAIN',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'WSPM',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'dateInt',}, axis = 1, inplace = True)\n",
        "# dataset.drop({'AQI_numeric',}, axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udjT5ULcZYUB"
      },
      "outputs": [],
      "source": [
        "# #number of rows and columns in the dataset\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUsf_dBHEFzM"
      },
      "source": [
        "Just doing `df.dropna()` drops all the NaN values only for the current execution of the cell. If you do the above `df.isnull().sum()` now, you can see that null values still persists. You can solve this by assigning the obtained output of\n",
        "`df.dopna()` to the variable `df` which stores our data (dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHQFwd23EFzN"
      },
      "outputs": [],
      "source": [
        "# correlations = dataset.corr()\n",
        "# fig, ax = plt.subplots(figsize=(15,15))\n",
        "# sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
        "# plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xACBpw54FQNW"
      },
      "outputs": [],
      "source": [
        "# g = sns.pairplot(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLe_4GaPqbXZ"
      },
      "outputs": [],
      "source": [
        "# dataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False, figsize=(30,20))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBTKpDgf2MLa"
      },
      "outputs": [],
      "source": [
        "# values = dataset.values\n",
        "# # integer encode direction\n",
        "# encoder = LabelEncoder()\n",
        "# values[:,4] = encoder.fit_transform(values[:,4])\n",
        "# print(values)\n",
        "# pd.DataFrame(values).to_excel('/content/drive/MyDrive/AQI/IntergerEncoding.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UinK_Nrr0L5g"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # ensure all data is float\n",
        "# values = values.astype('float32')\n",
        "# np.savetxt('/content/drive/MyDrive/AQI/my_data.csv', values, delimiter=',')\n",
        "# # values.to_csv(r'/content/drive/MyDrive/AQI/my_data.csv', index=False)\n",
        "# print(values)\n",
        "\n",
        "# pd.DataFrame(values).to_excel('/content/drive/MyDrive/AQI/FloatConversion.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x70GNxZ71H7w"
      },
      "outputs": [],
      "source": [
        "# reframed = series_to_supervised(values, 1, 1)\n",
        "# # split into train and test sets\n",
        "# values = reframed.values\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scaled_features = scaler.fit_transform(values[:,:-1])\n",
        "# scaled_label = scaler.fit_transform(values[:,-1].reshape(-1,1))\n",
        "# values = np.column_stack((scaled_features, scaled_label))\n",
        "# print(values)\n",
        "# pd.DataFrame(values).to_excel('/content/drive/MyDrive/AQI/MinMaxScaling.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQdyySccqfsH"
      },
      "outputs": [],
      "source": [
        "values = dataset.values\n",
        "# integer encode direction\n",
        "encoder = LabelEncoder()\n",
        "values[:,4] = encoder.fit_transform(values[:,4])\n",
        "# ensure all data is float\n",
        "values = values.astype('float32')\n",
        "reframed = series_to_supervised(values, 1, 1)\n",
        "reframed.drop(reframed.columns[[-7,-6,-5,-4,-3,-2,-1]], axis=1, inplace=True)\n",
        "print(reframed.head())\n",
        "\n",
        "# split into train and test sets\n",
        "values = reframed.values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_features = scaler.fit_transform(values[:,:-1])\n",
        "scaled_label = scaler.fit_transform(values[:,-1].reshape(-1,1))\n",
        "values = np.column_stack((scaled_features, scaled_label))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJJJvhrNqjYp"
      },
      "outputs": [],
      "source": [
        "n_train_hours = 365 * 24 * 40\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]\n",
        "# split into input and outputs\n",
        "# features take all values except the var1\n",
        "train_X, train_y = train[:, :-1], train[:, -1]\n",
        "test_X, test_y = test[:, :-1], test[:, -1]\n",
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OkXP3nyJOmf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnbEc08mBBrc"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(17,8))\n",
        "plt.plot(train_y, label='train_y', color='#0096FF',)\n",
        "plt.plot(test_y, label='test_y',color='#1F51FF',)\n",
        "\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "plt.savefig(\"traintest.pdf\")\n",
        "files.download(\"traintest.pdf\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0HmkIGttNP0"
      },
      "outputs": [],
      "source": [
        "# model = tf.keras.Sequential([keras.layers.Dense(units=10, input_shape=[2, 1]),\n",
        "#                              keras.layers.Dense(units=5, input_shape=[1]),\n",
        "#                              keras.layers.Dense(units=1, input_shape=[1])]) #ANN Part\n",
        "\n",
        "# model.add(LSTM(128, input_shape=(X_split_train[1], X_split_train[2])))\n",
        "# model.add(Dense(50, activation='tanh'))\n",
        "# model.add(Dense(1))\n",
        "# #keras.optimizers.SGD(lr=0.001, decay=1e-5, momentum=1.0, nesterov=False)\n",
        "# #keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True) #good\n",
        "\n",
        "# #keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9)\n",
        "\n",
        "# keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, amsgrad=False)\n",
        "# model.compile(loss='mae', optimizer='adam', metrics=['accuracy'] )\n",
        "# tf.keras.utils.plot_model(model=model, show_shapes=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7G0rxqs5niT"
      },
      "outputs": [],
      "source": [
        "# design network\n",
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=10, input_shape=[1, 18]),\n",
        "                             keras.layers.Dense(units=5, input_shape=[1]),\n",
        "                             keras.layers.Dense(units=1, input_shape=[1])]) #ANN Part\n",
        "\n",
        "model.add(LSTM(128, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(100, activation='tanh'))\n",
        "model.add(Dense(1))\n",
        "#keras.optimizers.SGD(lr=0.001, decay=1e-5, momentum=1.0, nesterov=False)\n",
        "#keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True) #good\n",
        "\n",
        "#keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9)\n",
        "\n",
        "keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, amsgrad=False)\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['accuracy'] )\n",
        "tf.keras.utils.plot_model(model=model, show_shapes=True)\n",
        "tf.keras.utils.plot_model(model=model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2A5ppdW8F5w"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNe81oRKqnLQ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "# # fit network\n",
        "history = model.fit(train_X, train_y, epochs=10, validation_data=(test_X, test_y),\n",
        "                    verbose=1, shuffle=False)\n",
        "\n",
        "# plot history\n",
        "pyplot.figure(figsize=(17,8))\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# make a prediction\n",
        "yhat = model.predict(test_X)\n",
        "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "# invert scaling for forecast\n",
        "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat = inv_yhat[:,0]\n",
        "# invert scaling for actual\n",
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y = inv_y[:,0]\n",
        "end = time.time()\n",
        "print('This took {} seconds.'.format(end - start))\n",
        "# calculate RMSE\n",
        "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "mae = tf.keras.metrics.mean_absolute_error(inv_y, inv_yhat)\n",
        "inv_y = scaler.inverse_transform(inv_y.reshape(-1,1))\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat.reshape(-1,1))\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8lnrwyAvyXp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "# calculate matrics\n",
        "mse = mean_squared_error(test_y, yhat)\n",
        "mae = mean_absolute_error(test_y, yhat)\n",
        "mape = mean_absolute_percentage_error(test_y, yhat)\n",
        "\n",
        "\n",
        "print('MSE: %.5f' % mse)\n",
        "print('MAE : %.5f' % mae)\n",
        "print('MAPE : %.5f' % mape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJhr5-PuFZgL"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(test_y, yhat)\n",
        "print('MSE: %.5f' % mse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G12nNg2Tq6tL"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='#0096FF', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='#1F51FF', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"Predict.pdf\")\n",
        "    files.download(\"Predict.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "plot_predicted(inv_yhat, inv_y)\n",
        "print('Root Mean Squared Error: {:.4f}'.format(rmse))\n",
        "print(\"R2 score : %.2f\" % r2_score(inv_y,inv_yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baIkgOjaasMH"
      },
      "outputs": [],
      "source": [
        "# summarize history for loss\n",
        "pyplot.figure(figsize=(17,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.savefig(\"loss.pdf\")\n",
        "files.download(\"loss.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuwZ9tXRtUnZ"
      },
      "outputs": [],
      "source": [
        "t=history.history['val_loss']\n",
        "np.average(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxEIDVMJbpZk"
      },
      "outputs": [],
      "source": [
        "plt.plot(yhat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBFkaY3IbsRt"
      },
      "outputs": [],
      "source": [
        "plt.plot(test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWsX8Oo-bPNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(\"Accuracy.pdf\")\n",
        "files.download(\"Accuracy.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdBEImpVzVu4"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x=dataset['PM2.5'], y=dataset['PM10'], data = dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX_zRzSMrTJM"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x=dataset['PM2.5'], y=dataset['SO2'], data = dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7zX27-Irouu"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x=dataset['PM2.5'], y=dataset['NO2'], data = dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhpHscEcrr-o"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x=dataset['PM2.5'], y=dataset['CO'], data = dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95YhTVoZwhxf"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x=dataset['PM2.5'], y=dataset['O3'], data = dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWRdFfaXtZrY"
      },
      "outputs": [],
      "source": [
        "# loading the data into the dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/AQI/Data.csv', header=0, index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_PVi-bhEFzO"
      },
      "outputs": [],
      "source": [
        "#defining training and testing data\n",
        "x_train = df[:350400]\n",
        "y_train = x_train['PM2.5']\n",
        "x_test = df[350400:418946]\n",
        "y_test = x_test['PM2.5']\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRYKbPREEFzP"
      },
      "source": [
        "There are many pollutants. Let's first try to predict PM2.5 concentration values. Let the years 2016 and 2017 be the testing set. As you can see below, these 2 years account for 21.9% of the data (test set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1pagJ_OEFzQ"
      },
      "outputs": [],
      "source": [
        "df.loc[350400:418946].count() / df.shape[0] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rkTh591EFzQ"
      },
      "outputs": [],
      "source": [
        "#Normalizing training data\n",
        "train_norm = x_train['PM2.5']\n",
        "\n",
        "#converted into array as all the methods available are for arrays and not lists\n",
        "train_norm_arr = np.asarray(train_norm)\n",
        "train_norm = np.reshape(train_norm_arr, (-1, 1))\n",
        "\n",
        "#Scaling all values between 0 and 1 so that large values don't just dominate\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train_norm = scaler.fit_transform(train_norm)\n",
        "for i in range(5):\n",
        "    print(train_norm[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfVvUpNiEFzR"
      },
      "source": [
        "Even after normalization and scaing, null values are possible (many people disregard this). Let's check if any null values are present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u123Gkw7EFzS"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in range(len(train_norm)):\n",
        "    if train_norm[i] == 0:\n",
        "        count = count +1\n",
        "print('Number of null values in train_norm = ', count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ9GlTG1EFzT"
      },
      "outputs": [],
      "source": [
        "#removing null values\n",
        "train_norm = train_norm[train_norm!=0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRG5-QrPEFzU"
      },
      "outputs": [],
      "source": [
        "#Normalizing testing data and repeating the same process as done for training data\n",
        "test_norm = x_test['PM2.5']\n",
        "test_norm_arr = np.asarray(test_norm)\n",
        "test_norm = np.reshape(test_norm_arr, (-1, 1))\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "test_norm = scaler.fit_transform(test_norm)\n",
        "for i in range(5):\n",
        "    print(test_norm[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dGflQXGEFzU"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in range(len(test_norm)):\n",
        "    if test_norm[i] == 0:\n",
        "        count = count + 1\n",
        "print('Number of null values in test_norm = ', count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTdZtsZ5EFzV"
      },
      "outputs": [],
      "source": [
        "#removing null values\n",
        "test_norm = test_norm[test_norm != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN2WCnWTEFzV"
      },
      "outputs": [],
      "source": [
        "print(train_norm.shape)\n",
        "print(test_norm.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fxGH6nREFzW"
      },
      "source": [
        "Since this is a time series data, we should be predicting the values after looking at a set of values rather than just a single value like we usually do. This takes into account the correlation between the data points and the timestamps. Because the neighbours should be considered for how the values change over time. Let's define a function to do this.\n",
        "\n",
        "The below function called split_sequence splits the sequence into sets of n values. This n is given as n_steps (step_size). For example, if n=3, we split the sequence in groups of 3. We create 2 empty lists and append the split sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epEMs4SpEFzX"
      },
      "outputs": [],
      "source": [
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(X),array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRe1iS39EFzY"
      },
      "source": [
        "Here the number of features = 1 as we will be predicting a single value. Let's reshape the split sequences into the format of number of rows, number of columns. (shape[0], shape[1]). In the output, we can see that groups of 3 since n_steps = 3 have been obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgNqy0eiEFzY"
      },
      "outputs": [],
      "source": [
        "n_steps = 2\n",
        "X_split_train, y_split_train = split_sequence(train_norm, n_steps)\n",
        "#for i in range(len(X_split_train)):\n",
        "    #print(X_split_train[i], y_split_train[i])\n",
        "n_features = 1\n",
        "X_split_train = X_split_train.reshape((X_split_train.shape[0], X_split_train.shape[1], n_features))\n",
        "for i in range(5):\n",
        "    print(X_split_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u61ElG4S7QkA"
      },
      "outputs": [],
      "source": [
        "print (X_split_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40lT11kCEFzZ"
      },
      "source": [
        "You can see below that, we predict the value for the first 3 values, then consider that output as one of the 3 values in the next set. For example, we preedict 0.1 first, then we take that 0.1 as input in the second set and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QOGDR4pEFzZ"
      },
      "outputs": [],
      "source": [
        "X_split_test, y_split_test = split_sequence(test_norm, n_steps)\n",
        "for i in range(5):\n",
        "    print(X_split_test[i], y_split_test[i])\n",
        "n_features = 1\n",
        "X_split_test = X_split_test.reshape((X_split_test.shape[0], X_split_test.shape[1], n_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5BLxo9F3-um"
      },
      "outputs": [],
      "source": [
        "print (X_split_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kso3Ozz9EFza"
      },
      "source": [
        "Let's define our neural network (LSTM: Long Short Term Memory). Let's add 50 nodes in our first layer with a ReLU (Rectified linear unit) activation. Their shape will be step size, number of features. Then we will add, a dense layer with one node for the output.\n",
        "\n",
        "We can try out different optimizers to see which minimizes loss and maximizes accuracy. Stochastic gradient descent (SGD), Adam, AdaBoost, RMSProp are few of them. lr = learning rate, decay = by how much to decay the learning rate, momentum = how much should the gradient descent be accelerated to dampen oscillations, nesterov = whether to use nesterov momentum. Nesterov has stronger convergence for convex functions. And then we compile using MSE (mean squared loss) as our loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of6ldD_b5mhQ"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im51TjTJEFza"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([keras.layers.Dense(units=10, input_shape=[2, 1]),\n",
        "                             keras.layers.Dense(units=5, input_shape=[1]),\n",
        "                             keras.layers.Dense(units=1, input_shape=[1])]) #ANN Part\n",
        "\n",
        "model.add(LSTM(128, input_shape=(X_split_train[1], X_split_train[2])))\n",
        "model.add(Dense(100, activation='tanh'))\n",
        "model.add(Dense(1))\n",
        "#keras.optimizers.SGD(lr=0.001, decay=1e-5, momentum=1.0, nesterov=False)\n",
        "#keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True) #good\n",
        "\n",
        "#keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9)\n",
        "\n",
        "keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, amsgrad=False)\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['accuracy'] )\n",
        "tf.keras.utils.plot_model(model=model, show_shapes=True)\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyoygOG9EFzb"
      },
      "outputs": [],
      "source": [
        "# fit model\n",
        "hist = model.fit(X_split_train, y_split_train, validation_data=(X_split_test, y_split_test), epochs=10, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1ROuJPvEFzb"
      },
      "outputs": [],
      "source": [
        "print(hist.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1wGDrT9EFzb"
      },
      "outputs": [],
      "source": [
        "yhat = model.predict(X_split_test)\n",
        "for i in range(5):\n",
        "    print(yhat[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnW9jww5EFzc"
      },
      "outputs": [],
      "source": [
        "# mse = mean_squared_error(y_split_test, yhat)\n",
        "# print('MSE: %.5f' % mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "murfR86e8vUP"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='red', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='green', linewidth='2')\n",
        "    plt.legend()\n",
        "    # plt.savefig(\"Pre.pdf\")\n",
        "    # files.download(\"Pre.pdf\")\n",
        "    plt.show()\n",
        "mse = mean_squared_error(y_split_test, yhat)\n",
        "print('MSE: %.5f' % mse)\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])\n",
        "print('Root Mean Squared Error: {:.4f}'.format(mse))\n",
        "print(\"R2 score : %.2f\" % r2_score(y_split_test,yhat))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv2HX4y3dIe0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "# calculate matrics\n",
        "mse = mean_squared_error(y_split_test, yhat)\n",
        "mae = mean_absolute_error(y_split_test, yhat)\n",
        "mape = mean_absolute_percentage_error(y_split_test, yhat)\n",
        "\n",
        "\n",
        "print('MSE: %.5f' % mse)\n",
        "print('MAE : %.5f' % mae)\n",
        "print('MAPE : %.5f' % mape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs8iWLYGdS-Z"
      },
      "outputs": [],
      "source": [
        "#defining training and testing data\n",
        "x_train = df[:335156 ]\n",
        "y_train = x_train['PM10']\n",
        "x_test = df[335156:418946]\n",
        "y_test = x_test['PM10']\n",
        "print(y_test)\n",
        "df.loc[335156:418946].count() / df.shape[0] * 100\n",
        "#Normalizing training data\n",
        "train_norm = x_train['PM10']\n",
        "\n",
        "#converted into array as all the methods available are for arrays and not lists\n",
        "train_norm_arr = np.asarray(train_norm)\n",
        "train_norm = np.reshape(train_norm_arr, (-1, 1))\n",
        "\n",
        "#Scaling all values between 0 and 1 so that large values don't just dominate\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train_norm = scaler.fit_transform(train_norm)\n",
        "for i in range(5):\n",
        "    print(train_norm[i])\n",
        "\n",
        "count = 0\n",
        "for i in range(len(train_norm)):\n",
        "    if train_norm[i] == 0:\n",
        "        count = count +1\n",
        "print('Number of null values in train_norm = ', count)\n",
        "#removing null values\n",
        "train_norm = train_norm[train_norm!=0]\n",
        "#Normalizing testing data and repeating the same process as done for training data\n",
        "test_norm = x_test['PM10']\n",
        "test_norm_arr = np.asarray(test_norm)\n",
        "test_norm = np.reshape(test_norm_arr, (-1, 1))\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "test_norm = scaler.fit_transform(test_norm)\n",
        "for i in range(5):\n",
        "    print(test_norm[i])\n",
        "\n",
        "count = 0\n",
        "for i in range(len(test_norm)):\n",
        "    if test_norm[i] == 0:\n",
        "        count = count + 1\n",
        "print('Number of null values in test_norm = ', count)\n",
        "#removing null values\n",
        "test_norm = test_norm[test_norm != 0]\n",
        "print(train_norm.shape)\n",
        "print(test_norm.shape)\n",
        "\n",
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(X),array(y)\n",
        "\n",
        "n_steps = 2\n",
        "X_split_train, y_split_train = split_sequence(train_norm, n_steps)\n",
        "#for i in range(len(X_split_train)):\n",
        "    #print(X_split_train[i], y_split_train[i])\n",
        "n_features = 1\n",
        "X_split_train = X_split_train.reshape((X_split_train.shape[0], X_split_train.shape[1], n_features))\n",
        "for i in range(5):\n",
        "    print(X_split_train)\n",
        "\n",
        "print (X_split_train.shape)\n",
        "\n",
        "X_split_test, y_split_test = split_sequence(test_norm, n_steps)\n",
        "for i in range(5):\n",
        "    print(X_split_test[i], y_split_test[i])\n",
        "n_features = 1\n",
        "X_split_test = X_split_test.reshape((X_split_test.shape[0], X_split_test.shape[1], n_features))\n",
        "\n",
        "print (X_split_test.shape)\n",
        "\n",
        "hist = model.fit(X_split_train, y_split_train, validation_data=(X_split_test, y_split_test), epochs=10, verbose = 1)\n",
        "\n",
        "print(hist.history.keys())\n",
        "\n",
        "yhat = model.predict(X_split_test)\n",
        "for i in range(5):\n",
        "    print(yhat[i])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lcvkwH66oXAJ"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"PreAc.pdf\")\n",
        "    files.download(\"PreAc.pdf\")\n",
        "    plt.show()\n",
        "mse = mean_squared_error(y_split_test, yhat)\n",
        "print('MSE: %.5f' % mse)\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])\n",
        "print('Root Mean Squared Error: {:.4f}'.format(mse))\n",
        "print(\"R2 score : %.2f\" % r2_score(y_split_test,yhat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Xh3hLPEFzc"
      },
      "source": [
        "Below, I have plotted the actual true values (first plot) and preedicted values (second plot). One can visually see that the distribution is almost the same. This says that our predictions are very accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gcz5nZM1EFzd"
      },
      "outputs": [],
      "source": [
        "# plt.plot(yhat)\n",
        "# plt.plot(y_split_test)\n",
        "# # summarize history for accuracy\n",
        "# plt.plot(hist.history['accuracy'])\n",
        "# plt.plot(hist.history['val_accuracy'])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper left')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a1OSyfEXEFzd"
      },
      "outputs": [],
      "source": [
        "# plt.plot(y_split_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QJISnkfKEFze"
      },
      "outputs": [],
      "source": [
        "# _, train_acc = model.evaluate(X_split_train, y_split_train, verbose=0)\n",
        "# _, test_acc = model.evaluate(X_split_test, y_split_test, verbose=0)\n",
        "# print('Train: %.5f, Test: %.5f' % (train_acc, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XxYxa21REFze"
      },
      "outputs": [],
      "source": [
        "# # summarize history for accuracy\n",
        "# plt.plot(hist.history['accuracy'])\n",
        "# plt.plot(hist.history['val_accuracy'])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper left')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BgtBZudEFze"
      },
      "source": [
        "Above, accuracy increase a lot in the last few epochs. Below, the loss gradually decrease. These are positive signs that our model is doing very good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jqh2UMQpEFzf"
      },
      "outputs": [],
      "source": [
        "# # summarize history for loss\n",
        "# plt.plot(hist.history['loss'])\n",
        "# plt.plot(hist.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper left')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGqBrNAEFzf"
      },
      "source": [
        "Until now, we just ran our model for prediction of a single pollutant. We have 6 pollutants in our dataset and can make predictions for all of them. So, I have made a function which can be used to predict the other pollutants rather than having to write the code again and again. I have commented the function calls. You can fork this kernel to uncomment and predit the other pollutants (Coz it would take up a lot of space and time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YLRixmF7EFzf"
      },
      "outputs": [],
      "source": [
        "def compute(var):\n",
        "    train_norm = x_train[var]\n",
        "    train_norm_arr = np.asarray(train_norm)\n",
        "    train_norm = np.reshape(train_norm_arr, (-1, 1))\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    train_norm = scaler.fit_transform(train_norm)\n",
        "    train_norm = train_norm[train_norm != 0]\n",
        "\n",
        "    test_norm = x_test[var]\n",
        "    test_norm_arr = np.asarray(test_norm)\n",
        "    test_norm = np.reshape(test_norm_arr, (-1, 1))\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    test_norm = scaler.fit_transform(test_norm)\n",
        "    test_norm = test_norm[test_norm != 0]\n",
        "\n",
        "    X_split_train, y_split_train = split_sequence(train_norm, n_steps)\n",
        "    X_split_train = X_split_train.reshape((X_split_train.shape[0], X_split_train.shape[1], n_features))\n",
        "\n",
        "    X_split_test, y_split_test = split_sequence(test_norm, n_steps)\n",
        "    X_split_test = X_split_test.reshape((X_split_test.shape[0], X_split_test.shape[1], n_features))\n",
        "\n",
        "    hist = model.fit(X_split_train, y_split_train, validation_data=(X_split_test, y_split_test), epochs=10, verbose = 1)\n",
        "\n",
        "    yhat = model.predict(X_split_test)\n",
        "\n",
        "    mse = mean_squared_error(y_split_test, yhat)\n",
        "    print('MSE :' ,mse)\n",
        "    print('Root Mean Squared Error: {:.4f}'.format(mse))\n",
        "    print(\"R2 score : %.2f\" % r2_score(y_split_test,yhat))\n",
        "    # calculate matrics\n",
        "    mse = mean_squared_error(y_split_test, yhat)\n",
        "    mae = mean_absolute_error(y_split_test, yhat)\n",
        "    mape = mean_absolute_percentage_error(y_split_test, yhat)\n",
        "\n",
        "\n",
        "    print('MSE: %.5f' % mse)\n",
        "    print('MAE : %.5f' % mae)\n",
        "    print('MAPE : %.5f' % mape)\n",
        "\n",
        "    # plt.plot(hist.history['accuracy'])\n",
        "    # plt.plot(hist.history['val_accuracy'])\n",
        "    # plt.title('model accuracy')\n",
        "    # plt.ylabel('accuracy')\n",
        "    # plt.xlabel('epoch')\n",
        "    # plt.legend(['train', 'test'], loc='upper left')\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.plot(hist.history['loss'])\n",
        "    # plt.plot(hist.history['val_loss'])\n",
        "    # plt.title('model loss')\n",
        "    # plt.ylabel('loss')\n",
        "    # plt.xlabel('epoch')\n",
        "    # plt.legend(['train', 'test'], loc='upper left')\n",
        "    # plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LPuIxqrDBRJJ"
      },
      "outputs": [],
      "source": [
        "compute('PM2.5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OWvYh7seBaIV"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"PM2.5.pdf\")\n",
        "    files.download(\"PM2.5.pdf\")\n",
        "    plt.show()\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AquMEqJoEFzg"
      },
      "outputs": [],
      "source": [
        "compute('PM10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U_4uQFaoaZ88"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"PM10.pdf\")\n",
        "    files.download(\"PM10.pdf\")\n",
        "    plt.show()\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEe0GYT3EFzg"
      },
      "outputs": [],
      "source": [
        "compute('SO2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxpOQkZYcplc"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"SO2.pdf\")\n",
        "    files.download(\"SO2.pdf\")\n",
        "    plt.show()\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCMiB_ghEFzh"
      },
      "outputs": [],
      "source": [
        "compute('NO2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB8HdqS6cr3f"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"NO2.pdf\")\n",
        "    files.download(\"NO2.pdf\")\n",
        "    plt.show()\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8S5D2FXEFzh"
      },
      "outputs": [],
      "source": [
        "compute('CO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prF6IE87ctRj"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"CO.pdf\")\n",
        "    files.download(\"CO.pdf\")\n",
        "    plt.show()\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVjD4gEZEFzi"
      },
      "outputs": [],
      "source": [
        "compute('O3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d43BtaMocuar"
      },
      "outputs": [],
      "source": [
        "def plot_predicted(predicted_data, true_data):\n",
        "    fig, ax = plt.subplots(figsize=(17,8))\n",
        "    ax.set_title('Prediction vs. Actual after 10 epochs of training')\n",
        "    ax.plot(true_data, label='True Data', color='green', linewidth='3')\n",
        "\n",
        "    ax.plot(predicted_data, label='Prediction', color='orange', linewidth='2')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"O3.pdf\")\n",
        "    files.download(\"O3.pdf\")\n",
        "    plt.show()\n",
        "plot_predicted(yhat[:1000,], y_split_test[:1000,])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}